{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python_defaultSpec_1595885172422",
      "display_name": "Python 3.8.2 64-bit"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dog-Cat-Rabbit Image Classifier\n",
        "\n",
        "#### The objective is to create an image classifier that can accurately differentiate between dogs, cats, and rabbits. I picked these animals because I happened to have 10000 images of dogs and cats and my girlfriend has a rabbit, ergo an easy test subject.\n",
        "\n",
        "#### As can be expected, we're going to be building a Convolutional Neural Network the specs of which are listed below:\n",
        "\n",
        "### Neural Net Specs\n",
        "\n",
        "#### Hidden Layer 1:\n",
        "filters=32, kernel_size=3, activation='relu', input_shape=[64,64,3], padding='same'\n",
        "pool_size=2, strides=1\n",
        "\n",
        "#### Hidden Layer 2:\n",
        "filters=64, kernel_size=3, activation='relu', padding='same'\n",
        "pool_size=2, strides=1\n",
        "\n",
        "#### Hidden Layer 3:\n",
        "filters=128, kernel_size=3, activation='relu', padding='same'\n",
        "pool_size=2, strides=1\n",
        "Dropout(0.25)\n",
        "\n",
        "#### Fully Connected Output Layer:\n",
        "units=512, activation='relu'\n",
        "Dropout(0.5)\n",
        "units=3, activation='softmax'\n",
        "optimizer = 'SGD', loss = 'categorical_crossentropy', metrics = ['accuracy']\n",
        "\n",
        "### Machine Specs:\n",
        "MacBook Pro (Retina, 13-inch, Late 2013) | 2.4 GHz Dual-Core Intel Core i5 | 8 GB 1600 MHz DDR3 | Intel Iris 1536 MB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufX7bBQutI0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras_preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GySWDlGVtYZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a0963302-fd50-423a-a81c-b66e565a4771"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'2.2.0'"
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RXQ2r-Btl-l",
        "colab_type": "text"
      },
      "source": [
        "# Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trhVaKPLtrb-",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pfOdoo8tqXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bccb9e59-dd87-478e-c25d-ff4913e23138",
        "tags": []
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255, # Applies feature scaling to our pixels\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True\n",
        "    \n",
        "    #rotation_range=40,\n",
        "    #width_shift_range=0.2,\n",
        "    #height_shift_range=0.2,\n",
        "    #fill_mode='nearest'\n",
        ")\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    '/Dog Cat Rabbit Classifier/dataset/training_set',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical' # This is binary if you're classifying between two classes\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found 12000 images belonging to 3 classes.\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUpFOuKE7gUH",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkvCCf206ihg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa51f3da-86c5-4bc9-b5df-be43001541cb",
        "tags": []
      },
      "source": [
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255, # Applies feature scaling to our pixels\n",
        ")\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    '/Dog Cat Rabbit Classifier/dataset/test_set',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found 3000 images belonging to 3 classes.\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP63kpvm_bBz",
        "colab_type": "text"
      },
      "source": [
        "# Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezH9IoWU_fUn",
        "colab_type": "text"
      },
      "source": [
        "## Initializing the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt8U29lt_icj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAlEraXD_o2c",
        "colab_type": "text"
      },
      "source": [
        "## Step 1 - Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "970jHNBu_q-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64,64,3], padding='same')) # the 3 refers to us using RGB vs 1 for greyscale\n",
        "# the Kernel size needs to be odd and determines the dimensions of our filter. If your images are larger than 128x128, then start with kernels of 5 or 7 then work you way down"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BODEyGuAN7R",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x38P53y1ANlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=1))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myKdpfZcAtl7",
        "colab_type": "text"
      },
      "source": [
        "## Adding a second convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_mQeMtTAxsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')) # We only use the input shape when we add our first layer\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=1))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Layers early in the network architecture (i.e., closer to the actual input image) learn fewer convolutional filters while layers deeper in the network (i.e., closer to the output predictions) will learn more filters.\n",
        "https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding a third convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same')) # We only use the input shape when we add our first layer\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=1))\n",
        "\n",
        "from keras.layers import Dropout\n",
        "\n",
        "cnn.add(Dropout(0.25))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten()) # Turns the output of the previous steps into a 1D vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbdiqT6FA_DH",
        "colab_type": "text"
      },
      "source": [
        "## Step 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQxVwQCDBDqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=512, activation='relu')) # Units: Number of hidden neurons connecting to this layer, you decide this\n",
        "cnn.add(Dropout(0.5))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5JvykN5BOOU",
        "colab_type": "text"
      },
      "source": [
        "## Step 5 - Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMU4d_ivBVVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=3, activation='softmax')) # Units = 1 if you're doing binary classification, and activation = sigmoid"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDivltH0BcW0",
        "colab_type": "text"
      },
      "source": [
        "# Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPKqgtIpBf-g",
        "colab_type": "text"
      },
      "source": [
        "## Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gj1h8FwBvdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn.compile(optimizer = 'SGD', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9fYN7nhBjZ8",
        "colab_type": "text"
      },
      "source": [
        "## Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61L1C4o0B4a7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bbfd34fa-43c6-490e-8e6d-7c40ee6c3e99",
        "tags": []
      },
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs=50, batch_size=64)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/50\n375/375 [==============================] - 1818s 5s/step - loss: 1.0403 - accuracy: 0.4565 - val_loss: 0.9755 - val_accuracy: 0.4953\nEpoch 2/50\n375/375 [==============================] - 1778s 5s/step - loss: 0.9736 - accuracy: 0.5203 - val_loss: 0.9144 - val_accuracy: 0.5603\nEpoch 3/50\n375/375 [==============================] - 2259s 6s/step - loss: 0.9257 - accuracy: 0.5545 - val_loss: 0.8605 - val_accuracy: 0.6097\nEpoch 4/50\n375/375 [==============================] - 1562s 4s/step - loss: 0.8822 - accuracy: 0.5906 - val_loss: 0.8320 - val_accuracy: 0.6210\nEpoch 5/50\n375/375 [==============================] - 1596s 4s/step - loss: 0.8440 - accuracy: 0.6153 - val_loss: 0.8247 - val_accuracy: 0.6163\nEpoch 6/50\n375/375 [==============================] - 1352s 4s/step - loss: 0.8074 - accuracy: 0.6373 - val_loss: 0.7984 - val_accuracy: 0.6313\nEpoch 7/50\n375/375 [==============================] - 1740s 5s/step - loss: 0.7765 - accuracy: 0.6565 - val_loss: 0.7606 - val_accuracy: 0.6590\nEpoch 8/50\n375/375 [==============================] - 1623s 4s/step - loss: 0.7484 - accuracy: 0.6715 - val_loss: 0.7922 - val_accuracy: 0.6493\nEpoch 9/50\n375/375 [==============================] - 1413s 4s/step - loss: 0.7270 - accuracy: 0.6804 - val_loss: 0.7044 - val_accuracy: 0.6940\nEpoch 10/50\n375/375 [==============================] - 1299s 3s/step - loss: 0.7054 - accuracy: 0.6940 - val_loss: 0.6938 - val_accuracy: 0.6917\nEpoch 11/50\n375/375 [==============================] - 1323s 4s/step - loss: 0.6834 - accuracy: 0.7029 - val_loss: 0.6789 - val_accuracy: 0.7067\nEpoch 12/50\n375/375 [==============================] - 1339s 4s/step - loss: 0.6695 - accuracy: 0.7125 - val_loss: 0.7169 - val_accuracy: 0.6853\nEpoch 13/50\n375/375 [==============================] - 1303s 3s/step - loss: 0.6440 - accuracy: 0.7243 - val_loss: 0.6799 - val_accuracy: 0.7067\nEpoch 14/50\n375/375 [==============================] - 1264s 3s/step - loss: 0.6286 - accuracy: 0.7358 - val_loss: 0.6816 - val_accuracy: 0.7080\nEpoch 15/50\n375/375 [==============================] - 1299s 3s/step - loss: 0.6093 - accuracy: 0.7428 - val_loss: 0.6324 - val_accuracy: 0.7223\nEpoch 16/50\n375/375 [==============================] - 1354s 4s/step - loss: 0.5928 - accuracy: 0.7492 - val_loss: 0.6744 - val_accuracy: 0.7117\nEpoch 17/50\n375/375 [==============================] - 1384s 4s/step - loss: 0.5774 - accuracy: 0.7592 - val_loss: 0.6449 - val_accuracy: 0.7273\nEpoch 18/50\n375/375 [==============================] - 1328s 4s/step - loss: 0.5615 - accuracy: 0.7645 - val_loss: 0.6521 - val_accuracy: 0.7263\nEpoch 19/50\n375/375 [==============================] - 1347s 4s/step - loss: 0.5389 - accuracy: 0.7778 - val_loss: 0.6700 - val_accuracy: 0.7173\nEpoch 20/50\n375/375 [==============================] - 1322s 4s/step - loss: 0.5267 - accuracy: 0.7845 - val_loss: 0.6614 - val_accuracy: 0.7273\nEpoch 21/50\n375/375 [==============================] - 1375s 4s/step - loss: 0.5086 - accuracy: 0.7922 - val_loss: 0.6810 - val_accuracy: 0.7163\nEpoch 22/50\n375/375 [==============================] - 1345s 4s/step - loss: 0.4899 - accuracy: 0.8037 - val_loss: 0.6080 - val_accuracy: 0.7520\nEpoch 23/50\n375/375 [==============================] - 1282s 3s/step - loss: 0.4645 - accuracy: 0.8132 - val_loss: 0.6299 - val_accuracy: 0.7403\nEpoch 24/50\n375/375 [==============================] - 1291s 3s/step - loss: 0.4607 - accuracy: 0.8117 - val_loss: 0.6274 - val_accuracy: 0.7463\nEpoch 25/50\n375/375 [==============================] - 1325s 4s/step - loss: 0.4358 - accuracy: 0.8229 - val_loss: 0.6224 - val_accuracy: 0.7490\nEpoch 26/50\n375/375 [==============================] - 1292s 3s/step - loss: 0.4273 - accuracy: 0.8305 - val_loss: 0.6390 - val_accuracy: 0.7503\nEpoch 27/50\n375/375 [==============================] - 1376s 4s/step - loss: 0.4011 - accuracy: 0.8418 - val_loss: 0.5855 - val_accuracy: 0.7663\nEpoch 28/50\n375/375 [==============================] - 1273s 3s/step - loss: 0.3865 - accuracy: 0.8497 - val_loss: 0.5823 - val_accuracy: 0.7687\nEpoch 29/50\n375/375 [==============================] - 1332s 4s/step - loss: 0.3751 - accuracy: 0.8517 - val_loss: 0.6096 - val_accuracy: 0.7567\nEpoch 30/50\n375/375 [==============================] - 1408s 4s/step - loss: 0.3585 - accuracy: 0.8618 - val_loss: 0.5966 - val_accuracy: 0.7603\nEpoch 31/50\n375/375 [==============================] - 1362s 4s/step - loss: 0.3482 - accuracy: 0.8639 - val_loss: 0.5992 - val_accuracy: 0.7703\nEpoch 32/50\n375/375 [==============================] - 1333s 4s/step - loss: 0.3277 - accuracy: 0.8741 - val_loss: 0.6262 - val_accuracy: 0.7673\nEpoch 33/50\n375/375 [==============================] - 1359s 4s/step - loss: 0.3187 - accuracy: 0.8807 - val_loss: 0.6490 - val_accuracy: 0.7593\nEpoch 34/50\n375/375 [==============================] - 1480s 4s/step - loss: 0.3025 - accuracy: 0.8864 - val_loss: 0.6055 - val_accuracy: 0.7763\nEpoch 35/50\n375/375 [==============================] - 1364s 4s/step - loss: 0.2940 - accuracy: 0.8852 - val_loss: 0.6269 - val_accuracy: 0.7780\nEpoch 36/50\n375/375 [==============================] - 1276s 3s/step - loss: 0.2746 - accuracy: 0.8987 - val_loss: 0.6238 - val_accuracy: 0.7767\nEpoch 37/50\n375/375 [==============================] - 1277s 3s/step - loss: 0.2647 - accuracy: 0.9028 - val_loss: 0.6288 - val_accuracy: 0.7780\nEpoch 38/50\n375/375 [==============================] - 1423s 4s/step - loss: 0.2560 - accuracy: 0.9022 - val_loss: 0.6139 - val_accuracy: 0.7873\nEpoch 39/50\n375/375 [==============================] - 1444s 4s/step - loss: 0.2438 - accuracy: 0.9082 - val_loss: 0.6205 - val_accuracy: 0.7863\nEpoch 40/50\n375/375 [==============================] - 1362s 4s/step - loss: 0.2344 - accuracy: 0.9150 - val_loss: 0.6804 - val_accuracy: 0.7613\nEpoch 41/50\n375/375 [==============================] - 1343s 4s/step - loss: 0.2244 - accuracy: 0.9193 - val_loss: 0.5817 - val_accuracy: 0.7907\nEpoch 42/50\n375/375 [==============================] - 1321s 4s/step - loss: 0.2094 - accuracy: 0.9225 - val_loss: 0.6595 - val_accuracy: 0.7823\nEpoch 43/50\n375/375 [==============================] - 1309s 3s/step - loss: 0.2086 - accuracy: 0.9226 - val_loss: 0.6568 - val_accuracy: 0.7827\nEpoch 44/50\n375/375 [==============================] - 1318s 4s/step - loss: 0.2035 - accuracy: 0.9226 - val_loss: 0.6088 - val_accuracy: 0.7833\nEpoch 45/50\n375/375 [==============================] - 1402s 4s/step - loss: 0.1846 - accuracy: 0.9309 - val_loss: 0.6497 - val_accuracy: 0.7870\nEpoch 46/50\n375/375 [==============================] - 1346s 4s/step - loss: 0.1798 - accuracy: 0.9362 - val_loss: 0.6552 - val_accuracy: 0.7910\nEpoch 47/50\n375/375 [==============================] - 1316s 4s/step - loss: 0.1719 - accuracy: 0.9377 - val_loss: 0.6361 - val_accuracy: 0.7913\nEpoch 48/50\n375/375 [==============================] - 1356s 4s/step - loss: 0.1661 - accuracy: 0.9389 - val_loss: 0.6940 - val_accuracy: 0.7830\nEpoch 49/50\n375/375 [==============================] - 1398s 4s/step - loss: 0.1643 - accuracy: 0.9394 - val_loss: 0.6860 - val_accuracy: 0.7940\nEpoch 50/50\n375/375 [==============================] - 1345s 4s/step - loss: 0.1687 - accuracy: 0.9414 - val_loss: 0.6612 - val_accuracy: 0.7917\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fb73d8c4c70>"
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKZhDPxGBo1B",
        "colab_type": "text"
      },
      "source": [
        "# Part 4 - Making a single prediction\n",
        "\n",
        "In the below predictions\n",
        "Lola = Dog,\n",
        "Gia = Dog,\n",
        "CJ = Dog,\n",
        "Luke = Dog,\n",
        "Bun = Rabbit,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[('Cat', ['Unknown', 0.74151987]),\n ('Cat', ['Rabbit', 0.9310766]),\n ('Cat', ['Cat', 0.9337303]),\n ('Dog', ['Dog', 0.99079365]),\n ('Dog', ['Dog', 0.9992816]),\n ('Rabbit', ['Rabbit', 0.99857974]),\n ('Dog', ['Dog', 0.99999654]),\n ('Bun', ['Rabbit', 0.9753357]),\n ('Bun', ['Cat', 0.8601471]),\n ('Cat', ['Unknown', 0.43681675]),\n ('Cat', ['Cat', 0.9655444]),\n ('Cat', ['Cat', 0.9993455]),\n ('Gia', ['Dog', 0.96553046]),\n ('Rabbit', ['Rabbit', 0.9999423]),\n ('Luke', ['Dog', 0.8733952])]"
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "results = []\n",
        "\n",
        "index = [\"Cat\", \"Dog\", \"Rabbit\"]\n",
        "\n",
        "filepath = \"/Dog Cat Rabbit Classifier/dataset/single_prediction\"\n",
        "\n",
        "files = os.listdir(filepath)\n",
        "files.remove(\".DS_Store\")\n",
        "files.sort()\n",
        "\n",
        "for i in files:\n",
        "    test_image = image.load_img(filepath + \"/\" + i, target_size=(64, 64))\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    test_image = test_image/255\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "    \n",
        "    result = cnn.predict(test_image)\n",
        "    probability = result[0].max()\n",
        "    \n",
        "    if probability > 0.8:\n",
        "        prediction = index[np.where(result[0] == result[0].max())[0][0]]\n",
        "    \n",
        "    else:\n",
        "        prediction = \"Unknown\"\n",
        "\n",
        "    results.append([prediction, probability])\n",
        "\n",
        "test_predictions = list(zip([i[4:-4] for i in files], results))\n",
        "test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "INFO:tensorflow:Assets written to: dog_cat_rabbit_classifier/assets\n"
        }
      ],
      "source": [
        "#cnn.save(\"dog_cat_rabbit_classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}